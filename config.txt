[preprocessing]
path_to_original_data=/media/alex/data2/GPT2_data/ThePile/pile/train/
path_to_store_preprocessed_data=/media/alex/data1/GPT2_data/data/
pile_sets_to_use=FreeLaw|HackerNews|Books3|OpenWebText2|Gutenberg (PG-19)|Pile-CC|BookCorpus2|Wikipedia (en)|PubMed Abstracts
max_len_words=256
min_len_words=16
max_len_tokens=512
max_num_of_tokens_in_batch=2100

[train]
device=cuda:0
epochs=1
num_of_chunks_to_use=17
num_workers=2
prefetch_factor=10
max_lr=2.5e-4
start_predicting_from_what_token=8
P_drop=0.1
init_std = 0.02
masking_minus_inf=-1e+6
disable_pin_memory=False
folder_to_save_state_dicts=saved_models/run
warmup_steps=40000
steps_to_attenuate_lr_to_zero=11500000
weight_decay=0.01

[inference]
device=cuda:0
chunk_num=16
how_many_tokens_to_generate=10
inference_method=greedy_search
beam_size=4
length_penalty=0.6

[val]
data_from_what_chunk_to_use=29
chunk_num_val=290
num_of_docs=10000

[architecture]
N=12
d_model=768
d_ff=3072
h=12
d_k=64
d_v=64
positional_encoding_max_pos=513
